{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting the project "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the API key \n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Its there\n"
     ]
    }
   ],
   "source": [
    "# checking whether its loaded into the variable or not \n",
    "if google_api_key ==\"\":\n",
    "    print(\"API key not found\")\n",
    "\n",
    "else:\n",
    "    print(\"Its there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIzaSyCq4rv48jawKPaIIFoNgfdP6ydYXZsArag\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ.get(\"GOOGLE_API_KEY\"))  # If this prints the key, it's set in the system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GENAI-2\\Gemini\\Q&A_with_gemini&Llama_index\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# importing the libraries from llama-index \n",
    "\n",
    "# importing the gemini using llama-index\n",
    "from llama_index.llms.gemini import Gemini\n",
    "\n",
    "# importing google genai \n",
    "import google.generativeai as genai \n",
    "\n",
    "# fetching directory reader from llama-index \n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# import vector store \n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# importing markdown and the disply method which will present the output in the clan format\n",
    "from IPython.display import Markdown,display\n",
    "\n",
    "# service context which is kind of the storage which will help in the RAG \n",
    "from llama_index.core import Settings\n",
    "\n",
    "# storagecontext and the load_index_storage \n",
    "from llama_index.core import StorageContext,load_index_from_storage\n",
    "\n",
    "# import embeddings \n",
    "from llama_index.embeddings.gemini import GeminiEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the configuration and the whether i am able to hit the API or not  \n",
    "\n",
    "genai.configure(api_key=google_api_key)\n",
    "\n",
    "# trying to list the models offered by the Gemini \n",
    "#for models in genai.list_models():\n",
    "    #print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/learnlm-1.5-pro-experimental\n",
      "models/gemma-3-27b-it\n"
     ]
    }
   ],
   "source": [
    "# Listing only the generatecontent models\n",
    "for models in genai.list_models():\n",
    "    if \"generateContent\" in models.supported_generation_methods:\n",
    "        print(models.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.readers.file.base.SimpleDirectoryReader at 0x204d018f880>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the document \n",
    "document = SimpleDirectoryReader(r\"D:\\GENAI-2\\Gemini\\Q&A_with_gemini&Llama_index\\Data\")\n",
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data stored in to the document \n",
    "data = document.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model \n",
    "Settings.llm = Gemini(model='models/gemini-1.5-pro',api_key=google_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading embedding model \n",
    "Settings.embed_model = GeminiEmbedding(model_name=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "Settings.transformations = [SentenceSplitter(chunk_size=800,chunk_overlap=20)] # this should be list always "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x204bc086e20>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing the vectors in local space\n",
    "index.storage_context.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the query engine over the index \n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning is a branch of artificial intelligence (AI) and computer science that uses data and algorithms to simulate human learning, gradually improving its accuracy.  It's a key component of data science, using statistical methods to train algorithms for making classifications or predictions, and uncovering insights in data mining projects.  These insights then influence decision-making in applications and businesses, impacting key growth metrics.  Machine learning algorithms are often created with frameworks like Python and platforms such as TensorFlow or PyTorch.  Deep learning, a sub-field of machine learning, uses artificial neural networks with multiple layers to learn from unstructured data, automatically determining distinguishing features.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 1\n",
    "response = query_engine.query(\"What is machine learning ?\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinforcement machine learning uses trial and error to learn, rather than training with sample data like supervised learning.  A reinforcement learning model is reinforced by successful outcomes to create the best recommendation or policy for a given problem.  Supervised learning, on the other hand, uses labeled datasets to train algorithms for data classification or outcome prediction.  The model adjusts its weights as input data is entered until it is properly fitted.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 2\n",
    "response = query_engine.query(\"What is the difference between Reinforcement machine learning and Supervised Machine learnign ?\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document does not contain information about Mahendra Singh Dhoni. This document discusses machine learning, including its history, relationship to other AI concepts, and platform considerations.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query 3: Asking the out of doc question \n",
    "response = query_engine.query(\"Who is Mahendra Singh Dhoni ?\")\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **We are successfully able to query the document**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
